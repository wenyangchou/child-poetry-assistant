{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82328db0-9b72-48e7-8a0f-a7da4ef25673",
   "metadata": {},
   "source": [
    "# 下载Qwen-7B-Chat\n",
    "\n",
    "- 方式一（正规方式）\n",
    "\n",
    "地址：https://huggingface.co/Qwen/Qwen1.5-7B-Chat\n",
    "\n",
    "注意：要翻墙\n",
    "\n",
    "- 方式二（国内特供方式）\n",
    "\n",
    "终端执行\n",
    "\n",
    "```\n",
    "git clone https://www.modelscope.cn/ccyh123/Qwen-7B-Chat.git\n",
    "```\n",
    "\n",
    "量化版本\n",
    "\n",
    "```\n",
    "git clone https://www.modelscope.cn/qwen/Qwen-7B-Chat-Int4.git\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c16fbb-456b-47a4-b190-c8faf7e56195",
   "metadata": {},
   "source": [
    "# 安装Intel依赖\n",
    "\n",
    "```\n",
    "pip install intel-extension-for-transformers\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "709644d6-9062-4154-b7c1-3f7bcecd1c56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T16:57:24.622000Z",
     "iopub.status.busy": "2024-06-01T16:57:24.621665Z",
     "iopub.status.idle": "2024-06-01T16:57:46.225387Z",
     "shell.execute_reply": "2024-06-01T16:57:46.224760Z",
     "shell.execute_reply.started": "2024-06-01T16:57:24.621981Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正克隆到 'Qwen-7B-Chat-Int4'...\n",
      "remote: Enumerating objects: 200, done.\u001b[K\n",
      "remote: Counting objects: 100% (133/133), done.\u001b[K\n",
      "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
      "remote: Total 200 (delta 48), reused 97 (delta 30), pack-reused 67\u001b[K\n",
      "接收对象中: 100% (200/200), 3.38 MiB | 46.71 MiB/s, 完成.\n",
      "处理 delta 中: 100% (66/66), 完成.\n",
      "过滤内容: 100% (3/3), 5.45 GiB | 267.22 MiB/s, 完成.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://www.modelscope.cn/qwen/Qwen-7B-Chat-Int4.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ccd3fd-70b3-4b63-971d-896077e9e0f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T06:27:31.918733Z",
     "iopub.status.busy": "2024-06-02T06:27:31.918400Z",
     "iopub.status.idle": "2024-06-02T06:29:07.520433Z",
     "shell.execute_reply": "2024-06-02T06:29:07.519858Z",
     "shell.execute_reply.started": "2024-06-02T06:27:31.918713Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Collecting intel-extension-for-transformers\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/9d/81/1053e18663de1eca0d3e185524ef197f9a6a91aabc7e0e04383a0910694c/intel_extension_for_transformers-1.4.2-cp38-cp38-manylinux_2_28_x86_64.whl (45.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from intel-extension-for-transformers) (23.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from intel-extension-for-transformers) (1.24.3)\n",
      "Collecting schema (from intel-extension-for-transformers)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/ad/1b/81855a88c6db2b114d5b2e9f96339190d5ee4d1b981d217fa32127bb00e0/schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from intel-extension-for-transformers) (6.0.1)\n",
      "Collecting neural-compressor (from intel-extension-for-transformers)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/8f/c5/4b1539af15b05b9533b61560e834a635c2ea7244234905cfa1ece6d77c14/neural_compressor-2.5.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (from intel-extension-for-transformers) (4.35.0)\n",
      "Collecting deprecated>=1.2.13 (from neural-compressor->intel-extension-for-transformers)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/20/8d/778b7d51b981a96554f29136cd59ca7880bf58094338085bcf2a979a0e6a/Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.8/site-packages (from neural-compressor->intel-extension-for-transformers) (4.8.0.76)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from neural-compressor->intel-extension-for-transformers) (2.0.3)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from neural-compressor->intel-extension-for-transformers) (10.0.0)\n",
      "Requirement already satisfied: prettytable in /opt/conda/lib/python3.8/site-packages (from neural-compressor->intel-extension-for-transformers) (3.9.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from neural-compressor->intel-extension-for-transformers) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.8/site-packages (from neural-compressor->intel-extension-for-transformers) (9.0.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from neural-compressor->intel-extension-for-transformers) (2.31.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from neural-compressor->intel-extension-for-transformers) (1.3.0)\n",
      "Requirement already satisfied: pycocotools in /opt/conda/lib/python3.8/site-packages (from neural-compressor->intel-extension-for-transformers) (2.0.7)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers->intel-extension-for-transformers) (3.12.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.8/site-packages (from transformers->intel-extension-for-transformers) (0.17.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers->intel-extension-for-transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.8/site-packages (from transformers->intel-extension-for-transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.8/site-packages (from transformers->intel-extension-for-transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers->intel-extension-for-transformers) (4.65.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.8/site-packages (from deprecated>=1.2.13->neural-compressor->intel-extension-for-transformers) (1.15.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers->intel-extension-for-transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers->intel-extension-for-transformers) (4.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.8/site-packages (from pandas->neural-compressor->intel-extension-for-transformers) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->neural-compressor->intel-extension-for-transformers) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.8/site-packages (from pandas->neural-compressor->intel-extension-for-transformers) (2023.3)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prettytable->neural-compressor->intel-extension-for-transformers) (0.2.6)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.8/site-packages (from pycocotools->neural-compressor->intel-extension-for-transformers) (3.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->neural-compressor->intel-extension-for-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->neural-compressor->intel-extension-for-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->neural-compressor->intel-extension-for-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->neural-compressor->intel-extension-for-transformers) (2023.5.7)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->neural-compressor->intel-extension-for-transformers) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->neural-compressor->intel-extension-for-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->neural-compressor->intel-extension-for-transformers) (3.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel-extension-for-transformers) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel-extension-for-transformers) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel-extension-for-transformers) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel-extension-for-transformers) (3.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->neural-compressor->intel-extension-for-transformers) (1.16.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: schema, deprecated, neural-compressor, intel-extension-for-transformers\n",
      "Successfully installed deprecated-1.2.14 intel-extension-for-transformers-1.4.2 neural-compressor-2.5.1 schema-0.7.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install intel-extension-for-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f2b0ec-2a53-425f-85d1-4f8fbb1465fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T06:29:12.476991Z",
     "iopub.status.busy": "2024-06-02T06:29:12.476649Z",
     "iopub.status.idle": "2024-06-02T06:29:29.348042Z",
     "shell.execute_reply": "2024-06-02T06:29:29.347485Z",
     "shell.execute_reply.started": "2024-06-02T06:29:12.476970Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Collecting neural-speed\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/0b/8a/9f304beba4925e352a739a1e72c3f9171a9432c5f054d32d3c00f256cfe7/neural_speed-1.0-cp38-cp38-manylinux_2_28_x86_64.whl (23.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: neural-speed\n",
      "Successfully installed neural-speed-1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install neural-speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3573bfda-8479-4f0a-a58f-b7dd44c1597e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from intel_extension_for_transformers.transformers import AutoModelForCausalLM\n",
    "model_name = \"./Qwen-7B-Chat-Int4\"     # Hugging Face model_id or local model\n",
    "prompt = \"Hello, who are you?\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True)\n",
    "outputs = model.generate(inputs, streamer=streamer, max_new_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d1856db-6c9f-4aa2-922b-1822bb3b19b5",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-06-01T17:04:55.317385Z",
     "iopub.status.busy": "2024-06-01T17:04:55.317050Z",
     "iopub.status.idle": "2024-06-01T17:04:56.145064Z",
     "shell.execute_reply": "2024-06-01T17:04:56.144429Z",
     "shell.execute_reply.started": "2024-06-01T17:04:55.317367Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intel-extension-for-transformers 1.3\n"
     ]
    }
   ],
   "source": [
    "!pip list |grep intel-extension-for-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70ace13b-0042-4cb0-be50-9ce3c3081f43",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-06-01T17:05:00.326575Z",
     "iopub.status.busy": "2024-06-01T17:05:00.326185Z",
     "iopub.status.idle": "2024-06-01T17:05:00.359832Z",
     "shell.execute_reply": "2024-06-01T17:05:00.359247Z",
     "shell.execute_reply.started": "2024-06-01T17:05:00.326527Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'WeightOnlyQuantConfig' from 'intel_extension_for_transformers.transformers' (/opt/conda/lib/python3.10/site-packages/intel_extension_for_transformers/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mintel_extension_for_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, WeightOnlyQuantConfig\n\u001b[1;32m      2\u001b[0m model_name_or_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Qwen-7B-Chat-Int4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# weight_dtype: int8/int4, compute_dtype: int8/fp32\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'WeightOnlyQuantConfig' from 'intel_extension_for_transformers.transformers' (/opt/conda/lib/python3.10/site-packages/intel_extension_for_transformers/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.transformers import AutoModelForCausalLM, WeightOnlyQuantConfig\n",
    "model_name_or_path = \"./Qwen-7B-Chat-Int4\"\n",
    "# weight_dtype: int8/int4, compute_dtype: int8/fp32\n",
    "woq_config = WeightOnlyQuantConfig(weight_dtype=\"int4\", compute_dtype=\"int8\",use_cache=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,quantization_config=woq_config, trust_remote_code=True)\n",
    "# inference\n",
    "from transformers import AutoTokenizer, TextStreamer\n",
    "prompt = \"Once upon a time, a little girl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "streamer = TextStreamer(tokenizer)\n",
    "outputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad643a9-a0f4-494c-b735-9b907384d3ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T16:44:52.640648Z",
     "iopub.status.busy": "2024-06-01T16:44:52.640177Z",
     "iopub.status.idle": "2024-06-01T16:45:27.114804Z",
     "shell.execute_reply": "2024-06-01T16:45:27.114151Z",
     "shell.execute_reply.started": "2024-06-01T16:44:52.640615Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 00:44:53,075 - modelscope - WARNING - Model revision not specified, use revision: v1.1.8\n",
      "2024-06-02 00:44:53 [INFO] cpu device is used.\n",
      "2024-06-02 00:44:53 [INFO] Applying Weight Only Quantization.\n",
      "2024-06-02 00:44:53 [INFO] Quantize model by Neural Speed with RTN Algorithm.\n",
      "2024-06-02 00:44:53,746 - modelscope - WARNING - Model revision not specified, use revision: v1.1.8\n",
      "2024-06-02 00:44:54,321 - modelscope - WARNING - Model revision not specified, use revision: v1.1.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd: ['python', PosixPath('/opt/conda/lib/python3.10/site-packages/neural_speed/convert/convert_qwen.py'), '--outfile', 'runtime_outs/ne_qwen_f32.bin', '--outtype', 'f32', '--model_hub', 'modelscope', 'qwen/Qwen-7B']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 00:44:56,094 - modelscope - INFO - PyTorch version 2.1.2+cpu Found.\n",
      "2024-06-02 00:44:56,096 - modelscope - INFO - TensorFlow version 2.14.0 Found.\n",
      "2024-06-02 00:44:56,096 - modelscope - INFO - Loading ast index from /mnt/workspace/.cache/modelscope/ast_indexer\n",
      "2024-06-02 00:44:56,124 - modelscope - INFO - Loading done! Current index file version is 1.14.0, with md5 aacbf9e8ebe525a5896d4c89570c0097 and a total number of 976 components indexed\n",
      "2024-06-02 00:44:57.109958: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-02 00:44:57.112228: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-02 00:44:57.142831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-02 00:44:57.142852: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-02 00:44:57.142871: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-02 00:44:57.148572: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-02 00:44:57.148754: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-02 00:44:57.890501: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-06-02 00:44:58,823 - modelscope - WARNING - Model revision not specified, use revision: v1.1.8\n",
      "Warning: please make sure that you are using the latest codes and checkpoints, especially if you used Qwen-7B before 09.25.2023.请使用最新模型和代码，尤其如果你在9月25日前已经开始使用Qwen-7B，千万注意不要使用错误代码和模型。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model:  qwen/Qwen-7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:23<00:00,  2.91s/it]\n",
      "2024-06-02 00:45:23,139 - modelscope - WARNING - Model revision not specified, use revision: v1.1.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_ne_hparams  0.hparams.n_vocab = 151936                        \n",
      "load_ne_hparams  1.hparams.n_embd = 4096                          \n",
      "load_ne_hparams  2.hparams.n_mult = 22016                         \n",
      "load_ne_hparams  3.hparams.n_head = 32                            \n",
      "load_ne_hparams  4.hparams.n_head_kv = 0                             \n",
      "load_ne_hparams  5.hparams.n_layer = 32                            \n",
      "load_ne_hparams  6.hparams.n_rot = 128                           \n",
      "load_ne_hparams  7.hparams.ftype = 0                             \n",
      "load_ne_hparams  8.hparams.max_seq_len = 8192                          \n",
      "load_ne_hparams  9.hparams.alibi_bias_max = 0.000                         \n",
      "load_ne_hparams  10.hparams.clip_qkv = 0.000                         \n",
      "load_ne_hparams  11.hparams.par_res = 0                             \n",
      "load_ne_hparams  12.hparams.word_embed_proj_dim = 0                             \n",
      "load_ne_hparams  13.hparams.do_layer_norm_before = 0                             \n",
      "load_ne_hparams  14.hparams.multi_query_group_num = 0                             \n",
      "load_ne_hparams  15.hparams.ffn_hidden_size = 11008                         \n",
      "load_ne_hparams  16.hparams.inner_hidden_size = 0                             \n",
      "load_ne_hparams  17.hparams.n_experts = 0                             \n",
      "load_ne_hparams  18.hparams.n_experts_used = 0                             \n",
      "load_ne_hparams  19.hparams.n_embd_head_k = 0                             \n",
      "load_ne_hparams  20.hparams.norm_eps = 0.000001                      \n",
      "load_ne_hparams  21.hparams.freq_base = 10000.000                     \n",
      "load_ne_hparams  22.hparams.freq_scale = 1.000                         \n",
      "load_ne_hparams  23.hparams.rope_scaling_factor = 0.000                         \n",
      "load_ne_hparams  24.hparams.original_max_position_embeddings = 0                             \n",
      "load_ne_hparams  25.hparams.use_yarn = 0                             \n",
      "load_ne_vocab    26.vocab.bos_token_id = 151643                        \n",
      "load_ne_vocab    27.vocab.eos_token_id = 151643                        \n",
      "load_ne_vocab    28.vocab.pad_token_id = -1                            \n",
      "load_ne_vocab    29.vocab.sep_token_id = -1                            \n",
      "ne_ftype: 10\n",
      "Loading the bin file with NE format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.cpp: loading model from runtime_outs/ne_qwen_f32.bin\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unexpectedly reached end of file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen/Qwen-7B\u001b[39m\u001b[38;5;124m\"\u001b[39m     \u001b[38;5;66;03m# Modelscope model_id or local model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnce upon a time, there existed a little girl,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodelscope\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/intel_extension_for_transformers/transformers/modeling/modeling_auto.py:722\u001b[0m, in \u001b[0;36m_BaseQBitsAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneural_speed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m    721\u001b[0m model \u001b[38;5;241m=\u001b[39m Model()\n\u001b[0;32m--> 722\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=E1123\u001b[39;49;00m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43malg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheme\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_ggml\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_ggml\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gptq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_awq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_hub\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m model\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m quantization_config\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/neural_speed/__init__.py:187\u001b[0m, in \u001b[0;36mModel.init\u001b[0;34m(self, model_name, use_quant, use_gptq, use_awq, use_autoround, weight_dtype, alg, group_size, scale_dtype, compute_dtype, use_ggml, model_hub)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP32 model will be used.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp32_bin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mout_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_bin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mweight_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m                              \u001b[49m\u001b[43malg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mgroup_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mscale_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mcompute_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m                              \u001b[49m\u001b[43muse_ggml\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_ggml\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(quant_bin), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFail to quantize model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# clean\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unexpectedly reached end of file"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "from modelscope import AutoTokenizer\n",
    "from intel_extension_for_transformers.transformers import AutoModelForCausalLM\n",
    "model_name = \"qwen/Qwen-7B\"     # Modelscope model_id or local model\n",
    "prompt = \"Once upon a time, there existed a little girl,\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, model_hub=\"modelscope\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "streamer = TextStreamer(tokenizer)\n",
    "outputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a6a090-45e5-40f2-b873-e6872531220d",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-06-01T16:47:47.726276Z",
     "iopub.status.busy": "2024-06-01T16:47:47.725959Z",
     "iopub.status.idle": "2024-06-01T16:48:08.654285Z",
     "shell.execute_reply": "2024-06-01T16:48:08.653459Z",
     "shell.execute_reply.started": "2024-06-01T16:47:47.726257Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 00:48:45,058 - modelscope - ERROR - Authentication token does not exist, failed to access model Intel/neural-chat-7b-v3-1 which may not exist or may be                 private. Please login first.\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "Response details: {'Code': 10010205001, 'Message': '获取模型版本失败，信息：record not found', 'RequestId': 'b1d7cc1a-eaa1-4bf1-9eb4-cca53c0a4cfe', 'Success': False}, Request id: 65cc86572cda40ea9481423ef083a169",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/modelscope/hub/errors.py:91\u001b[0m, in \u001b[0;36mhandle_http_response\u001b[0;34m(response, logger, cookies, model_id)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://www.modelscope.cn/api/v1/models/Intel/neural-chat-7b-v3-1/revisions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntel/neural-chat-7b-v3-1\u001b[39m\u001b[38;5;124m\"\u001b[39m     \u001b[38;5;66;03m# Hugging Face model_id or local model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnce upon a time, there existed a little girl,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodelscope\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/intel_extension_for_transformers/transformers/modeling/modeling_auto.py:494\u001b[0m, in \u001b[0;36m_BaseQBitsAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_hub \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelscope\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmodelscope\u001b[39;00m \u001b[38;5;66;03m# pylint: disable=E0401\u001b[39;00m\n\u001b[0;32m--> 494\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mmodelscope\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     config, _ \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    498\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    499\u001b[0m         return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    501\u001b[0m \n\u001b[1;32m    502\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/modelscope/utils/hf_util.py:105\u001b[0m, in \u001b[0;36mget_wrapped_class.<locals>.ClassWrapper.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pretrained_model_name_or_path):\n\u001b[1;32m    104\u001b[0m     revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m'\u001b[39m, DEFAULT_MODEL_REVISION)\n\u001b[0;32m--> 105\u001b[0m     model_dir \u001b[38;5;241m=\u001b[39m \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_file_pattern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_file_pattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     model_dir \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/modelscope/hub/snapshot_download.py:98\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(model_id, revision, cache_dir, user_agent, local_files_only, cookies, ignore_file_pattern)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cookies \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     cookies \u001b[38;5;241m=\u001b[39m ModelScopeConfig\u001b[38;5;241m.\u001b[39mget_cookies()\n\u001b[0;32m---> 98\u001b[0m revision_detail \u001b[38;5;241m=\u001b[39m \u001b[43m_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_valid_revision_detail\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m revision \u001b[38;5;241m=\u001b[39m revision_detail[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevision\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    102\u001b[0m snapshot_header \u001b[38;5;241m=\u001b[39m headers \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCI_TEST\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron \u001b[38;5;28;01melse\u001b[39;00m {\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mheaders,\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSnapshot\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    106\u001b[0m     }\n\u001b[1;32m    107\u001b[0m }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/modelscope/hub/api.py:497\u001b[0m, in \u001b[0;36mHubApi.get_valid_revision_detail\u001b[0;34m(self, model_id, revision, cookies)\u001b[0m\n\u001b[1;32m    493\u001b[0m current_timestamp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mtimestamp()))\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# for active development in library codes (non-release-branches), release_timestamp\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# is set to be a far-away-time-in-the-future, to ensure that we shall\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# get the master-HEAD version from model repo by default (when no revision is provided)\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m all_branches_detail, all_tags_detail \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_branches_and_tags_details\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m all_branches \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevision\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_branches_detail] \u001b[38;5;28;01mif\u001b[39;00m all_branches_detail \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    500\u001b[0m all_tags \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevision\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_tags_detail] \u001b[38;5;28;01mif\u001b[39;00m all_tags_detail \u001b[38;5;28;01melse\u001b[39;00m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/modelscope/hub/api.py:577\u001b[0m, in \u001b[0;36mHubApi.get_model_branches_and_tags_details\u001b[0;34m(self, model_id, use_cookies)\u001b[0m\n\u001b[1;32m    574\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/v1/models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/revisions\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    575\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mget(path, cookies\u001b[38;5;241m=\u001b[39mcookies,\n\u001b[1;32m    576\u001b[0m                      headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_headers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders))\n\u001b[0;32m--> 577\u001b[0m \u001b[43mhandle_http_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m d \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m    579\u001b[0m raise_on_error(d)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/modelscope/hub/errors.py:98\u001b[0m, in \u001b[0;36mhandle_http_response\u001b[0;34m(response, logger, cookies, model_id)\u001b[0m\n\u001b[1;32m     94\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthentication token does not exist, failed to access model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which may not exist or may be \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124m        private. Please login first.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     97\u001b[0m message \u001b[38;5;241m=\u001b[39m _decode_response_error(response)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponse details: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, Request id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m     99\u001b[0m                 (message, get_request_id(response))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m\n",
      "\u001b[0;31mHTTPError\u001b[0m: Response details: {'Code': 10010205001, 'Message': '获取模型版本失败，信息：record not found', 'RequestId': 'b1d7cc1a-eaa1-4bf1-9eb4-cca53c0a4cfe', 'Success': False}, Request id: 65cc86572cda40ea9481423ef083a169"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from intel_extension_for_transformers.transformers import AutoModelForCausalLM\n",
    "model_name = \"Intel/neural-chat-7b-v3-1\"     # Hugging Face model_id or local model\n",
    "prompt = \"Once upon a time, there existed a little girl,\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, model_hub=\"modelscope\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True)\n",
    "outputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe9055f-fc56-460e-aa7f-4655cc20f808",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 以下是按照官方视频给出的llama的demo\n",
    "\n",
    "- 下载llama模型\n",
    "\n",
    "```\n",
    "git clone https://www.modelscope.cn/shakechen/Llama-2-7b-chat-hf.git\n",
    "\n",
    "```\n",
    "\n",
    "- 安装环境\n",
    "\n",
    "```\n",
    "pip install intel-extension-for-transformers==1.3\n",
    "```\n",
    "\n",
    "- 运行\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b480c70-92de-4bab-b201-95ecf61b92d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T06:46:52.515435Z",
     "iopub.status.busy": "2024-06-02T06:46:52.515086Z",
     "iopub.status.idle": "2024-06-02T07:01:40.274768Z",
     "shell.execute_reply": "2024-06-02T07:01:40.274119Z",
     "shell.execute_reply.started": "2024-06-02T06:46:52.515412Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-02 14:46:59 [INFO] Applying Weight Only Quantization.\n",
      "2024-06-02 14:46:59 [INFO] Using LLM runtime.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd: ['python', PosixPath('/opt/conda/lib/python3.8/site-packages/intel_extension_for_transformers/llm/runtime/graph/scripts/convert_llama.py'), '--outfile', 'runtime_outs/ne_llama_f32.bin', '--outtype', 'f32', 'Llama-2-7b-chat-hf']\n",
      "Loading model file Llama-2-7b-chat-hf/model-00001-of-00002.safetensors\n",
      "Loading model file Llama-2-7b-chat-hf/model-00001-of-00002.safetensors\n",
      "Loading model file Llama-2-7b-chat-hf/model-00002-of-00002.safetensors\n",
      "Loading vocab file Llama-2-7b-chat-hf/tokenizer.model\n",
      "Writing vocab...\n",
      "[  1/291] Writing tensor tok_embeddings.weight                  | size  32000 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[  2/291] Writing tensor norm.weight                            | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[  3/291] Writing tensor output.weight                          | size  32000 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[  4/291] Writing tensor layers.0.attention.wq.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[  5/291] Writing tensor layers.0.attention.wk.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[  6/291] Writing tensor layers.0.attention.wv.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[  7/291] Writing tensor layers.0.attention.wo.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[  8/291] Writing tensor layers.0.attention_norm.weight         | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[  9/291] Writing tensor layers.0.feed_forward.w1.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 10/291] Writing tensor layers.0.feed_forward.w2.weight        | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[ 11/291] Writing tensor layers.0.feed_forward.w3.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 12/291] Writing tensor layers.0.ffn_norm.weight               | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 13/291] Writing tensor layers.1.attention.wq.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 14/291] Writing tensor layers.1.attention.wk.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 15/291] Writing tensor layers.1.attention.wv.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 16/291] Writing tensor layers.1.attention.wo.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 17/291] Writing tensor layers.1.attention_norm.weight         | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 18/291] Writing tensor layers.1.feed_forward.w1.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 19/291] Writing tensor layers.1.feed_forward.w2.weight        | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[ 20/291] Writing tensor layers.1.feed_forward.w3.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 21/291] Writing tensor layers.1.ffn_norm.weight               | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 22/291] Writing tensor layers.2.attention.wq.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 23/291] Writing tensor layers.2.attention.wk.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 24/291] Writing tensor layers.2.attention.wv.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 25/291] Writing tensor layers.2.attention.wo.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 26/291] Writing tensor layers.2.attention_norm.weight         | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 27/291] Writing tensor layers.2.feed_forward.w1.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 28/291] Writing tensor layers.2.feed_forward.w2.weight        | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[ 29/291] Writing tensor layers.2.feed_forward.w3.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 30/291] Writing tensor layers.2.ffn_norm.weight               | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 31/291] Writing tensor layers.3.attention.wq.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 32/291] Writing tensor layers.3.attention.wk.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 33/291] Writing tensor layers.3.attention.wv.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 34/291] Writing tensor layers.3.attention.wo.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 35/291] Writing tensor layers.3.attention_norm.weight         | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 36/291] Writing tensor layers.3.feed_forward.w1.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 37/291] Writing tensor layers.3.feed_forward.w2.weight        | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[ 38/291] Writing tensor layers.3.feed_forward.w3.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 39/291] Writing tensor layers.3.ffn_norm.weight               | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 40/291] Writing tensor layers.4.attention.wq.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 41/291] Writing tensor layers.4.attention.wk.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 42/291] Writing tensor layers.4.attention.wv.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 43/291] Writing tensor layers.4.attention.wo.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 44/291] Writing tensor layers.4.attention_norm.weight         | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 45/291] Writing tensor layers.4.feed_forward.w1.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 46/291] Writing tensor layers.4.feed_forward.w2.weight        | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[ 47/291] Writing tensor layers.4.feed_forward.w3.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 48/291] Writing tensor layers.4.ffn_norm.weight               | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 49/291] Writing tensor layers.5.attention.wq.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 50/291] Writing tensor layers.5.attention.wk.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 51/291] Writing tensor layers.5.attention.wv.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 52/291] Writing tensor layers.5.attention.wo.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 53/291] Writing tensor layers.5.attention_norm.weight         | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 54/291] Writing tensor layers.5.feed_forward.w1.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 55/291] Writing tensor layers.5.feed_forward.w2.weight        | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[ 56/291] Writing tensor layers.5.feed_forward.w3.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 57/291] Writing tensor layers.5.ffn_norm.weight               | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 58/291] Writing tensor layers.6.attention.wq.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 59/291] Writing tensor layers.6.attention.wk.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 60/291] Writing tensor layers.6.attention.wv.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 61/291] Writing tensor layers.6.attention.wo.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 62/291] Writing tensor layers.6.attention_norm.weight         | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 63/291] Writing tensor layers.6.feed_forward.w1.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 64/291] Writing tensor layers.6.feed_forward.w2.weight        | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[ 65/291] Writing tensor layers.6.feed_forward.w3.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 66/291] Writing tensor layers.6.ffn_norm.weight               | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 67/291] Writing tensor layers.7.attention.wq.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 68/291] Writing tensor layers.7.attention.wk.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 69/291] Writing tensor layers.7.attention.wv.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 70/291] Writing tensor layers.7.attention.wo.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 71/291] Writing tensor layers.7.attention_norm.weight         | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 72/291] Writing tensor layers.7.feed_forward.w1.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 73/291] Writing tensor layers.7.feed_forward.w2.weight        | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[ 74/291] Writing tensor layers.7.feed_forward.w3.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 75/291] Writing tensor layers.7.ffn_norm.weight               | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 76/291] Writing tensor layers.8.attention.wq.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 77/291] Writing tensor layers.8.attention.wk.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 78/291] Writing tensor layers.8.attention.wv.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 79/291] Writing tensor layers.8.attention.wo.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 80/291] Writing tensor layers.8.attention_norm.weight         | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 81/291] Writing tensor layers.8.feed_forward.w1.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 82/291] Writing tensor layers.8.feed_forward.w2.weight        | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[ 83/291] Writing tensor layers.8.feed_forward.w3.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 84/291] Writing tensor layers.8.ffn_norm.weight               | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 85/291] Writing tensor layers.9.attention.wq.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 86/291] Writing tensor layers.9.attention.wk.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 87/291] Writing tensor layers.9.attention.wv.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 88/291] Writing tensor layers.9.attention.wo.weight           | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 89/291] Writing tensor layers.9.attention_norm.weight         | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 90/291] Writing tensor layers.9.feed_forward.w1.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 91/291] Writing tensor layers.9.feed_forward.w2.weight        | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[ 92/291] Writing tensor layers.9.feed_forward.w3.weight        | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 93/291] Writing tensor layers.9.ffn_norm.weight               | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 94/291] Writing tensor layers.10.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 95/291] Writing tensor layers.10.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 96/291] Writing tensor layers.10.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 97/291] Writing tensor layers.10.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[ 98/291] Writing tensor layers.10.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[ 99/291] Writing tensor layers.10.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[100/291] Writing tensor layers.10.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[101/291] Writing tensor layers.10.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[102/291] Writing tensor layers.10.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[103/291] Writing tensor layers.11.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[104/291] Writing tensor layers.11.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[105/291] Writing tensor layers.11.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[106/291] Writing tensor layers.11.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[107/291] Writing tensor layers.11.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[108/291] Writing tensor layers.11.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[109/291] Writing tensor layers.11.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[110/291] Writing tensor layers.11.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[111/291] Writing tensor layers.11.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[112/291] Writing tensor layers.12.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[113/291] Writing tensor layers.12.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[114/291] Writing tensor layers.12.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[115/291] Writing tensor layers.12.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[116/291] Writing tensor layers.12.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[117/291] Writing tensor layers.12.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[118/291] Writing tensor layers.12.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[119/291] Writing tensor layers.12.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[120/291] Writing tensor layers.12.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[121/291] Writing tensor layers.13.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[122/291] Writing tensor layers.13.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[123/291] Writing tensor layers.13.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[124/291] Writing tensor layers.13.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[125/291] Writing tensor layers.13.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[126/291] Writing tensor layers.13.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[127/291] Writing tensor layers.13.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[128/291] Writing tensor layers.13.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[129/291] Writing tensor layers.13.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[130/291] Writing tensor layers.14.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[131/291] Writing tensor layers.14.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[132/291] Writing tensor layers.14.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[133/291] Writing tensor layers.14.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[134/291] Writing tensor layers.14.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[135/291] Writing tensor layers.14.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[136/291] Writing tensor layers.14.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[137/291] Writing tensor layers.14.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[138/291] Writing tensor layers.14.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[139/291] Writing tensor layers.15.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[140/291] Writing tensor layers.15.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[141/291] Writing tensor layers.15.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[142/291] Writing tensor layers.15.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[143/291] Writing tensor layers.15.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[144/291] Writing tensor layers.15.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[145/291] Writing tensor layers.15.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[146/291] Writing tensor layers.15.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[147/291] Writing tensor layers.15.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[148/291] Writing tensor layers.16.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[149/291] Writing tensor layers.16.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[150/291] Writing tensor layers.16.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[151/291] Writing tensor layers.16.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[152/291] Writing tensor layers.16.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[153/291] Writing tensor layers.16.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[154/291] Writing tensor layers.16.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[155/291] Writing tensor layers.16.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[156/291] Writing tensor layers.16.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[157/291] Writing tensor layers.17.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[158/291] Writing tensor layers.17.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[159/291] Writing tensor layers.17.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[160/291] Writing tensor layers.17.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[161/291] Writing tensor layers.17.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[162/291] Writing tensor layers.17.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[163/291] Writing tensor layers.17.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[164/291] Writing tensor layers.17.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[165/291] Writing tensor layers.17.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[166/291] Writing tensor layers.18.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[167/291] Writing tensor layers.18.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[168/291] Writing tensor layers.18.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[169/291] Writing tensor layers.18.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[170/291] Writing tensor layers.18.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[171/291] Writing tensor layers.18.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[172/291] Writing tensor layers.18.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[173/291] Writing tensor layers.18.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[174/291] Writing tensor layers.18.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[175/291] Writing tensor layers.19.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[176/291] Writing tensor layers.19.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[177/291] Writing tensor layers.19.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[178/291] Writing tensor layers.19.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[179/291] Writing tensor layers.19.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[180/291] Writing tensor layers.19.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[181/291] Writing tensor layers.19.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[182/291] Writing tensor layers.19.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[183/291] Writing tensor layers.19.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[184/291] Writing tensor layers.20.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[185/291] Writing tensor layers.20.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[186/291] Writing tensor layers.20.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[187/291] Writing tensor layers.20.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[188/291] Writing tensor layers.20.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[189/291] Writing tensor layers.20.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[190/291] Writing tensor layers.20.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[191/291] Writing tensor layers.20.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[192/291] Writing tensor layers.20.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[193/291] Writing tensor layers.21.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[194/291] Writing tensor layers.21.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[195/291] Writing tensor layers.21.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[196/291] Writing tensor layers.21.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[197/291] Writing tensor layers.21.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[198/291] Writing tensor layers.21.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[199/291] Writing tensor layers.21.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[200/291] Writing tensor layers.21.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[201/291] Writing tensor layers.21.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[202/291] Writing tensor layers.22.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[203/291] Writing tensor layers.22.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[204/291] Writing tensor layers.22.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[205/291] Writing tensor layers.22.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[206/291] Writing tensor layers.22.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[207/291] Writing tensor layers.22.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[208/291] Writing tensor layers.22.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[209/291] Writing tensor layers.22.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[210/291] Writing tensor layers.22.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[211/291] Writing tensor layers.23.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[212/291] Writing tensor layers.23.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[213/291] Writing tensor layers.23.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[214/291] Writing tensor layers.23.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[215/291] Writing tensor layers.23.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[216/291] Writing tensor layers.23.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[217/291] Writing tensor layers.23.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[218/291] Writing tensor layers.23.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[219/291] Writing tensor layers.23.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[220/291] Writing tensor layers.24.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[221/291] Writing tensor layers.24.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[222/291] Writing tensor layers.24.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[223/291] Writing tensor layers.24.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[224/291] Writing tensor layers.24.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[225/291] Writing tensor layers.24.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[226/291] Writing tensor layers.24.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[227/291] Writing tensor layers.24.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[228/291] Writing tensor layers.24.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[229/291] Writing tensor layers.25.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[230/291] Writing tensor layers.25.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[231/291] Writing tensor layers.25.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[232/291] Writing tensor layers.25.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[233/291] Writing tensor layers.25.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[234/291] Writing tensor layers.25.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[235/291] Writing tensor layers.25.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[236/291] Writing tensor layers.25.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[237/291] Writing tensor layers.25.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[238/291] Writing tensor layers.26.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[239/291] Writing tensor layers.26.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[240/291] Writing tensor layers.26.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[241/291] Writing tensor layers.26.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[242/291] Writing tensor layers.26.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[243/291] Writing tensor layers.26.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[244/291] Writing tensor layers.26.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[245/291] Writing tensor layers.26.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[246/291] Writing tensor layers.26.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[247/291] Writing tensor layers.27.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[248/291] Writing tensor layers.27.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[249/291] Writing tensor layers.27.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[250/291] Writing tensor layers.27.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[251/291] Writing tensor layers.27.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[252/291] Writing tensor layers.27.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[253/291] Writing tensor layers.27.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[254/291] Writing tensor layers.27.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[255/291] Writing tensor layers.27.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[256/291] Writing tensor layers.28.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[257/291] Writing tensor layers.28.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[258/291] Writing tensor layers.28.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[259/291] Writing tensor layers.28.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[260/291] Writing tensor layers.28.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[261/291] Writing tensor layers.28.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[262/291] Writing tensor layers.28.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[263/291] Writing tensor layers.28.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[264/291] Writing tensor layers.28.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[265/291] Writing tensor layers.29.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[266/291] Writing tensor layers.29.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[267/291] Writing tensor layers.29.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[268/291] Writing tensor layers.29.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[269/291] Writing tensor layers.29.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[270/291] Writing tensor layers.29.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[271/291] Writing tensor layers.29.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[272/291] Writing tensor layers.29.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[273/291] Writing tensor layers.29.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[274/291] Writing tensor layers.30.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[275/291] Writing tensor layers.30.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[276/291] Writing tensor layers.30.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[277/291] Writing tensor layers.30.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[278/291] Writing tensor layers.30.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[279/291] Writing tensor layers.30.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[280/291] Writing tensor layers.30.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[281/291] Writing tensor layers.30.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[282/291] Writing tensor layers.30.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[283/291] Writing tensor layers.31.attention.wq.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[284/291] Writing tensor layers.31.attention.wk.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[285/291] Writing tensor layers.31.attention.wv.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[286/291] Writing tensor layers.31.attention.wo.weight          | size   4096 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[287/291] Writing tensor layers.31.attention_norm.weight        | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "[288/291] Writing tensor layers.31.feed_forward.w1.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[289/291] Writing tensor layers.31.feed_forward.w2.weight       | size   4096 x  11008  |                  type UnquantizedDataType(name='F32')\n",
      "[290/291] Writing tensor layers.31.feed_forward.w3.weight       | size  11008 x   4096  |                  type UnquantizedDataType(name='F32')\n",
      "[291/291] Writing tensor layers.31.ffn_norm.weight              | size   4096           |                  type UnquantizedDataType(name='F32')\n",
      "Wrote runtime_outs/ne_llama_f32.bin\n",
      "ne_ftype: 10\n",
      "[   1/ 291]                tok_embeddings.weight -     4096 x 32000, type =    f32, 0_0_32_0_0,quantizing .. GGML size =   500.00 MB ->    70.31 MB\n",
      "\n",
      "[   2/ 291]                          norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[   3/ 291]                        output.weight -     4096 x 32000, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   500.00 MB ->    85.98 MB\n",
      "\n",
      "[   4/ 291]         layers.0.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[   5/ 291]         layers.0.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[   6/ 291]         layers.0.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[   7/ 291]         layers.0.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[   8/ 291]       layers.0.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[   9/ 291]      layers.0.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  10/ 291]      layers.0.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  11/ 291]      layers.0.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  12/ 291]             layers.0.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  13/ 291]         layers.1.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  14/ 291]         layers.1.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  15/ 291]         layers.1.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  16/ 291]         layers.1.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  17/ 291]       layers.1.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  18/ 291]      layers.1.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  19/ 291]      layers.1.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  20/ 291]      layers.1.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  21/ 291]             layers.1.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  22/ 291]         layers.2.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  23/ 291]         layers.2.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  24/ 291]         layers.2.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  25/ 291]         layers.2.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  26/ 291]       layers.2.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  27/ 291]      layers.2.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  28/ 291]      layers.2.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  29/ 291]      layers.2.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  30/ 291]             layers.2.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  31/ 291]         layers.3.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  32/ 291]         layers.3.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  33/ 291]         layers.3.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  34/ 291]         layers.3.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  35/ 291]       layers.3.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  36/ 291]      layers.3.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  37/ 291]      layers.3.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  38/ 291]      layers.3.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  39/ 291]             layers.3.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  40/ 291]         layers.4.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  41/ 291]         layers.4.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  42/ 291]         layers.4.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  43/ 291]         layers.4.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  44/ 291]       layers.4.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  45/ 291]      layers.4.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  46/ 291]      layers.4.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  47/ 291]      layers.4.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  48/ 291]             layers.4.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  49/ 291]         layers.5.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  50/ 291]         layers.5.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  51/ 291]         layers.5.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  52/ 291]         layers.5.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  53/ 291]       layers.5.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  54/ 291]      layers.5.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  55/ 291]      layers.5.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  56/ 291]      layers.5.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  57/ 291]             layers.5.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  58/ 291]         layers.6.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  59/ 291]         layers.6.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  60/ 291]         layers.6.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  61/ 291]         layers.6.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  62/ 291]       layers.6.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  63/ 291]      layers.6.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  64/ 291]      layers.6.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  65/ 291]      layers.6.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  66/ 291]             layers.6.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  67/ 291]         layers.7.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  68/ 291]         layers.7.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  69/ 291]         layers.7.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  70/ 291]         layers.7.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  71/ 291]       layers.7.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  72/ 291]      layers.7.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  73/ 291]      layers.7.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  74/ 291]      layers.7.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  75/ 291]             layers.7.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  76/ 291]         layers.8.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  77/ 291]         layers.8.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  78/ 291]         layers.8.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  79/ 291]         layers.8.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  80/ 291]       layers.8.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  81/ 291]      layers.8.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  82/ 291]      layers.8.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  83/ 291]      layers.8.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  84/ 291]             layers.8.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  85/ 291]         layers.9.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  86/ 291]         layers.9.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  87/ 291]         layers.9.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  88/ 291]         layers.9.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  89/ 291]       layers.9.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  90/ 291]      layers.9.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  91/ 291]      layers.9.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  92/ 291]      layers.9.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  93/ 291]             layers.9.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  94/ 291]        layers.10.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  95/ 291]        layers.10.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  96/ 291]        layers.10.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  97/ 291]        layers.10.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  98/ 291]      layers.10.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  99/ 291]     layers.10.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 100/ 291]     layers.10.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 101/ 291]     layers.10.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 102/ 291]            layers.10.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 103/ 291]        layers.11.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 104/ 291]        layers.11.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 105/ 291]        layers.11.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 106/ 291]        layers.11.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 107/ 291]      layers.11.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 108/ 291]     layers.11.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 109/ 291]     layers.11.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 110/ 291]     layers.11.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 111/ 291]            layers.11.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 112/ 291]        layers.12.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 113/ 291]        layers.12.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 114/ 291]        layers.12.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 115/ 291]        layers.12.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 116/ 291]      layers.12.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 117/ 291]     layers.12.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 118/ 291]     layers.12.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 119/ 291]     layers.12.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 120/ 291]            layers.12.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 121/ 291]        layers.13.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 122/ 291]        layers.13.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 123/ 291]        layers.13.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 124/ 291]        layers.13.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 125/ 291]      layers.13.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 126/ 291]     layers.13.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 127/ 291]     layers.13.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 128/ 291]     layers.13.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 129/ 291]            layers.13.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 130/ 291]        layers.14.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 131/ 291]        layers.14.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 132/ 291]        layers.14.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 133/ 291]        layers.14.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 134/ 291]      layers.14.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 135/ 291]     layers.14.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 136/ 291]     layers.14.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 137/ 291]     layers.14.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 138/ 291]            layers.14.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 139/ 291]        layers.15.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 140/ 291]        layers.15.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 141/ 291]        layers.15.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 142/ 291]        layers.15.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 143/ 291]      layers.15.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 144/ 291]     layers.15.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 145/ 291]     layers.15.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 146/ 291]     layers.15.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 147/ 291]            layers.15.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 148/ 291]        layers.16.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 149/ 291]        layers.16.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 150/ 291]        layers.16.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 151/ 291]        layers.16.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 152/ 291]      layers.16.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 153/ 291]     layers.16.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 154/ 291]     layers.16.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 155/ 291]     layers.16.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 156/ 291]            layers.16.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 157/ 291]        layers.17.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 158/ 291]        layers.17.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 159/ 291]        layers.17.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 160/ 291]        layers.17.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 161/ 291]      layers.17.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 162/ 291]     layers.17.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 163/ 291]     layers.17.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 164/ 291]     layers.17.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 165/ 291]            layers.17.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 166/ 291]        layers.18.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 167/ 291]        layers.18.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 168/ 291]        layers.18.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 169/ 291]        layers.18.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 170/ 291]      layers.18.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 171/ 291]     layers.18.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 172/ 291]     layers.18.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 173/ 291]     layers.18.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 174/ 291]            layers.18.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 175/ 291]        layers.19.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 176/ 291]        layers.19.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 177/ 291]        layers.19.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 178/ 291]        layers.19.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 179/ 291]      layers.19.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 180/ 291]     layers.19.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 181/ 291]     layers.19.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 182/ 291]     layers.19.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 183/ 291]            layers.19.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 184/ 291]        layers.20.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 185/ 291]        layers.20.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 186/ 291]        layers.20.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 187/ 291]        layers.20.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 188/ 291]      layers.20.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 189/ 291]     layers.20.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 190/ 291]     layers.20.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 191/ 291]     layers.20.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 192/ 291]            layers.20.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 193/ 291]        layers.21.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 194/ 291]        layers.21.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 195/ 291]        layers.21.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 196/ 291]        layers.21.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 197/ 291]      layers.21.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 198/ 291]     layers.21.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 199/ 291]     layers.21.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 200/ 291]     layers.21.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 201/ 291]            layers.21.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 202/ 291]        layers.22.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 203/ 291]        layers.22.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 204/ 291]        layers.22.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 205/ 291]        layers.22.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 206/ 291]      layers.22.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 207/ 291]     layers.22.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 208/ 291]     layers.22.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 209/ 291]     layers.22.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 210/ 291]            layers.22.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 211/ 291]        layers.23.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 212/ 291]        layers.23.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 213/ 291]        layers.23.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 214/ 291]        layers.23.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 215/ 291]      layers.23.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 216/ 291]     layers.23.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 217/ 291]     layers.23.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 218/ 291]     layers.23.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 219/ 291]            layers.23.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 220/ 291]        layers.24.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 221/ 291]        layers.24.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 222/ 291]        layers.24.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 223/ 291]        layers.24.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 224/ 291]      layers.24.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 225/ 291]     layers.24.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 226/ 291]     layers.24.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 227/ 291]     layers.24.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 228/ 291]            layers.24.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 229/ 291]        layers.25.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 230/ 291]        layers.25.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 231/ 291]        layers.25.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 232/ 291]        layers.25.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 233/ 291]      layers.25.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 234/ 291]     layers.25.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 235/ 291]     layers.25.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 236/ 291]     layers.25.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 237/ 291]            layers.25.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 238/ 291]        layers.26.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 239/ 291]        layers.26.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 240/ 291]        layers.26.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 241/ 291]        layers.26.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 242/ 291]      layers.26.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 243/ 291]     layers.26.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 244/ 291]     layers.26.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 245/ 291]     layers.26.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 246/ 291]            layers.26.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 247/ 291]        layers.27.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 248/ 291]        layers.27.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 249/ 291]        layers.27.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 250/ 291]        layers.27.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 251/ 291]      layers.27.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 252/ 291]     layers.27.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 253/ 291]     layers.27.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 254/ 291]     layers.27.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 255/ 291]            layers.27.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 256/ 291]        layers.28.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 257/ 291]        layers.28.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 258/ 291]        layers.28.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 259/ 291]        layers.28.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 260/ 291]      layers.28.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 261/ 291]     layers.28.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 262/ 291]     layers.28.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 263/ 291]     layers.28.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 264/ 291]            layers.28.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 265/ 291]        layers.29.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 266/ 291]        layers.29.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 267/ 291]        layers.29.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 268/ 291]        layers.29.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 269/ 291]      layers.29.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 270/ 291]     layers.29.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 271/ 291]     layers.29.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 272/ 291]     layers.29.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 273/ 291]            layers.29.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 274/ 291]        layers.30.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 275/ 291]        layers.30.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 276/ 291]        layers.30.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 277/ 291]        layers.30.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 278/ 291]      layers.30.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 279/ 291]     layers.30.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 280/ 291]     layers.30.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 281/ 291]     layers.30.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 282/ 291]            layers.30.ffn_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 283/ 291]        layers.31.attention.wq.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 284/ 291]        layers.31.attention.wk.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 285/ 291]        layers.31.attention.wv.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 286/ 291]        layers.31.attention.wo.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 287/ 291]      layers.31.attention_norm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 288/ 291]     layers.31.feed_forward.w1.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 289/ 291]     layers.31.feed_forward.w2.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 290/ 291]     layers.31.feed_forward.w3.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.cpp: loading model from runtime_outs/ne_llama_f32.bin\n",
      "model.cpp: saving model to runtime_outs/ne_llama_q_int4_jblas_cint8_g32.bin\n",
      "model.cpp: loading model from runtime_outs/ne_llama_q_int4_jblas_cint8_g32.bin\n",
      "init: n_vocab    = 32000\n",
      "init: n_embd     = 4096\n",
      "init: n_mult     = 256\n",
      "init: n_head     = 32\n",
      "init: n_head_kv  = 32\n",
      "init: n_layer    = 32\n",
      "init: n_rot      = 128\n",
      "init: n_ff       = 11008\n",
      "init: n_parts    = 1\n",
      "load: ne ctx size = 4427.36 MB\n",
      "load: mem required  = 6477.36 MB (+ memory per state)\n",
      "...................................................................................................\n",
      "model_init_from_file: support_jblas_kv = 0\n",
      "model_init_from_file: kv self size =  128.00 MB\n",
      "2024-06-02 14:48:18.266054: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-02 14:48:18.336352: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-02 14:48:18.852864: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-02 14:48:18.856320: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-02 14:48:20.173567: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Once upon a time, there existed a little girl, no more than 10 years old, who lived in a small village nestled in the rolling "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m streamer \u001b[38;5;241m=\u001b[39m TextStreamer(tokenizer)\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name,quantization_config\u001b[38;5;241m=\u001b[39mwoq_config, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/intel_extension_for_transformers/llm/runtime/graph/__init__.py:176\u001b[0m, in \u001b[0;36mModel.generate\u001b[0;34m(self, input_ids, streamer, interactive, ignore_prompt, stopping_criteria, **generate_kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m input_list \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     input_list \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# next-token stage will use previous output\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from intel_extension_for_transformers.transformers import AutoModelForCausalLM, WeightOnlyQuantConfig\n",
    "woq_config = WeightOnlyQuantConfig(compute_dtype=\"int8\",weight_dtype=\"int4\",use_cache=True)\n",
    "model_name = \"Llama-2-7b-chat-hf\"     # Hugging Face model_id or local model\n",
    "prompt = \"Once upon a time, there existed a little girl,\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=woq_config, trust_remote_code=True)\n",
    "outputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678b26d-2026-4b21-8a48-b35ca016ffb1",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-06-02T07:08:34.109216Z",
     "iopub.status.busy": "2024-06-02T07:08:34.108837Z",
     "iopub.status.idle": "2024-06-02T07:09:16.783458Z",
     "shell.execute_reply": "2024-06-02T07:09:16.782786Z",
     "shell.execute_reply.started": "2024-06-02T07:08:34.109191Z"
    },
    "tags": []
   },
   "source": [
    "# Qwen替换\n",
    "\n",
    "- 模型下载\n",
    "\n",
    "```\n",
    "git clone https://www.modelscope.cn/ccyh123/Qwen-7B-Chat.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91bebbff-7bd3-456e-be83-5398bc035d8b",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-06-02T07:18:26.870809Z",
     "iopub.status.busy": "2024-06-02T07:18:26.870514Z",
     "iopub.status.idle": "2024-06-02T07:19:26.949970Z",
     "shell.execute_reply": "2024-06-02T07:19:26.949374Z",
     "shell.execute_reply.started": "2024-06-02T07:18:26.870789Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-02 15:18:44 [INFO] Applying Weight Only Quantization.\n",
      "2024-06-02 15:18:44 [INFO] Using LLM runtime.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd: ['python', PosixPath('/opt/conda/lib/python3.10/site-packages/intel_extension_for_transformers/llm/runtime/graph/scripts/convert_qwen.py'), '--outfile', 'runtime_outs/ne_qwen_f32.bin', '--outtype', 'f32', 'Qwen-7B-Chat']\n",
      "Loading model:  Qwen-7B-Chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:32<00:00,  4.01s/it]\n",
      "2024-06-02 15:19:22.951229: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-02 15:19:22.953605: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-02 15:19:22.987334: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-02 15:19:22.987358: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-02 15:19:22.987383: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-02 15:19:22.993522: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-02 15:19:22.993715: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-02 15:19:23.717333: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "model.cpp: loading model from runtime_outs/ne_qwen_f32.bin\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unexpectedly reached end of file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m      9\u001b[0m streamer \u001b[38;5;241m=\u001b[39m TextStreamer(tokenizer)\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwoq_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(inputs, streamer\u001b[38;5;241m=\u001b[39mstreamer, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/intel_extension_for_transformers/transformers/modeling/modeling_auto.py:173\u001b[0m, in \u001b[0;36m_BaseQBitsAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mintel_extension_for_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m    172\u001b[0m     model \u001b[38;5;241m=\u001b[39m Model()\n\u001b[0;32m--> 173\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43malg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheme\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_ggml\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_ggml\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gptq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_gptq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/intel_extension_for_transformers/llm/runtime/graph/__init__.py:123\u001b[0m, in \u001b[0;36mModel.init\u001b[0;34m(self, model_name, use_quant, use_gptq, **quant_kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP32 model will be used.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp32_bin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_bin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mquant_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(quant_bin), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFail to quantize model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# clean\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unexpectedly reached end of file"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from intel_extension_for_transformers.transformers import AutoModelForCausalLM, WeightOnlyQuantConfig\n",
    "woq_config = WeightOnlyQuantConfig(compute_dtype=\"int8\",weight_dtype=\"int4\",use_cache=True)\n",
    "model_name = \"Qwen-7B-Chat\"     # Hugging Face model_id or local model\n",
    "prompt = \"Once upon a time, there existed a little girl,\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=woq_config, trust_remote_code=True)\n",
    "outputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a833b09-b559-4b24-b763-181dd424b81b",
   "metadata": {},
   "source": [
    "# 转换chatglm3-6b\n",
    "\n",
    "- 代码下载\n",
    "\n",
    "```\n",
    "git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a7c3b1-d74d-486d-b878-a43bb7591df4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T07:22:48.240896Z",
     "iopub.status.busy": "2024-06-02T07:22:48.240565Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "2024-06-02 15:22:48 [INFO] Applying Weight Only Quantization.\n",
      "2024-06-02 15:22:48 [INFO] Using LLM runtime.\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd: ['python', PosixPath('/opt/conda/lib/python3.10/site-packages/intel_extension_for_transformers/llm/runtime/graph/scripts/convert_chatglm.py'), '--outfile', 'runtime_outs/ne_chatglm_f32.bin', '--outtype', 'f32', 'chatglm3-6b']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00, 19.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLM-2 converting: \n",
      "transformer.embedding.word_embeddings.weight torch.Size([65024, 4096]) torch.float16\n",
      "transformer.rotary_pos_emb.inv_freq torch.Size([32]) torch.float16\n",
      "transformer.encoder.layers.0.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.0.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.0.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.0.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.0.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.0.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.0.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.1.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.1.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.1.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.1.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.1.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.1.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.1.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.2.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.2.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.2.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.2.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.2.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.2.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.2.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.3.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.3.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.3.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.3.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.3.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.3.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.3.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.4.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.4.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.4.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.4.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.4.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.4.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.4.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.5.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.5.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.5.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.5.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.5.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.5.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.5.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.6.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.6.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.6.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.6.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.6.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.6.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.6.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.7.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.7.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.7.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.7.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.7.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.7.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.7.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.8.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.8.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.8.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.8.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.8.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.8.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.8.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.9.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.9.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.9.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.9.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.9.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.9.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.9.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.10.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.10.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.10.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.10.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.10.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.10.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.10.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.11.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.11.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.11.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.11.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.11.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.11.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.11.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.12.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.12.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.12.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.12.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.12.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.12.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.12.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.13.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.13.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.13.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.13.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.13.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.13.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.13.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.14.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.14.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.14.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.14.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.14.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.14.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.14.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.15.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.15.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.15.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.15.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.15.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.15.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.15.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.16.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.16.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.16.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.16.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.16.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.16.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.16.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.17.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.17.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.17.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.17.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.17.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.17.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.17.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.18.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.18.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.18.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.18.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.18.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.18.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.18.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.19.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.19.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.19.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.19.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.19.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.19.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.19.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.20.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.20.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.20.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.20.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.20.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.20.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.20.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.21.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.21.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.21.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.21.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.21.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.21.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.21.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.22.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.22.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.22.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.22.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.22.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.22.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.22.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.23.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.23.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.23.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.23.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.23.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.23.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.23.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.24.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.24.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.24.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.24.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.24.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.24.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.24.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.25.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.25.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.25.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.25.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.25.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.25.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.25.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.26.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.26.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.26.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.26.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.26.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.26.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.26.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.layers.27.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.27.self_attention.query_key_value.weight torch.Size([4608, 4096]) torch.float16\n",
      "transformer.encoder.layers.27.self_attention.query_key_value.bias torch.Size([4608]) torch.float16\n",
      "transformer.encoder.layers.27.self_attention.dense.weight torch.Size([4096, 4096]) torch.float16\n",
      "transformer.encoder.layers.27.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.encoder.layers.27.mlp.dense_h_to_4h.weight torch.Size([27392, 4096]) torch.float16\n",
      "transformer.encoder.layers.27.mlp.dense_4h_to_h.weight torch.Size([4096, 13696]) torch.float16\n",
      "transformer.encoder.final_layernorm.weight torch.Size([4096]) torch.float16\n",
      "transformer.output_layer.weight torch.Size([65024, 4096]) torch.float16\n",
      "{'_name_or_path': 'THUDM/chatglm3-6b', 'model_type': 'chatglm', 'architectures': ['ChatGLMModel'], 'auto_map': {'AutoConfig': 'configuration_chatglm.ChatGLMConfig', 'AutoModel': 'modeling_chatglm.ChatGLMForConditionalGeneration', 'AutoModelForCausalLM': 'modeling_chatglm.ChatGLMForConditionalGeneration', 'AutoModelForSeq2SeqLM': 'modeling_chatglm.ChatGLMForConditionalGeneration', 'AutoModelForSequenceClassification': 'modeling_chatglm.ChatGLMForSequenceClassification'}, 'add_bias_linear': False, 'add_qkv_bias': True, 'apply_query_key_layer_scaling': True, 'apply_residual_connection_post_layernorm': False, 'attention_dropout': 0.0, 'attention_softmax_in_fp32': True, 'bias_dropout_fusion': True, 'ffn_hidden_size': 13696, 'fp32_residual_connection': False, 'hidden_dropout': 0.0, 'hidden_size': 4096, 'kv_channels': 128, 'layernorm_epsilon': 1e-05, 'multi_query_attention': True, 'multi_query_group_num': 2, 'num_attention_heads': 32, 'num_layers': 28, 'original_rope': True, 'padded_vocab_size': 65024, 'post_layer_norm': True, 'rmsnorm': True, 'seq_length': 8192, 'use_cache': True, 'torch_dtype': 'float16', 'transformers_version': '4.30.2', 'tie_word_embeddings': False, 'eos_token_id': 2, 'pad_token_id': 0}\n",
      "Loading vocab file chatglm3-6b/tokenizer.model\n",
      "Processing variable: transformer.embedding.word_embeddings.weight with shape:  (65024, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.rotary_pos_emb.inv_freq with shape:  (32,)\n",
      "Processing variable: transformer.encoder.layers.0.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.0.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.0.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.0.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.0.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.0.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.0.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.1.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.1.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.1.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.1.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.1.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.1.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.1.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.2.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.2.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.2.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.2.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.2.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.2.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.2.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.3.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.3.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.3.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.3.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.3.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.3.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.3.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.4.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.4.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.4.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.4.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.4.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.4.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.4.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.5.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.5.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.5.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.5.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.5.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.5.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.5.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.6.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.6.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.6.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.6.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.6.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.6.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.6.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.7.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.7.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.7.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.7.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.7.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.7.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.7.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.8.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.8.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.8.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.8.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.8.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.8.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.8.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.9.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.9.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.9.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.9.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.9.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.9.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.9.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.10.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.10.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.10.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.10.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.10.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.10.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.10.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.11.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.11.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.11.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.11.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.11.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.11.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.11.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.12.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.12.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.12.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.12.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.12.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.12.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.12.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.13.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.13.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.13.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.13.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.13.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.13.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.13.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.14.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.14.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.14.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.14.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.14.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.14.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.14.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.15.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.15.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.15.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.15.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.15.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.15.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.15.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.16.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.16.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.16.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.16.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.16.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.16.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.16.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.17.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.17.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.17.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.17.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.17.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.17.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.17.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.18.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.18.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.18.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.18.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.18.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.18.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.18.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.19.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.19.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.19.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.19.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.19.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.19.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.19.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.20.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.20.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.20.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.20.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.20.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.20.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.20.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.21.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.21.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.21.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.21.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.21.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.21.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.21.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.22.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.22.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.22.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.22.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.22.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.22.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.22.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.23.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.23.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.23.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.23.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.23.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.23.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.23.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.24.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.24.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.24.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.24.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.24.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.24.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.24.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.25.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.25.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.25.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.25.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.25.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.25.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.25.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.26.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.26.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.26.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.26.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.26.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.26.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.26.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.27.input_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.27.self_attention.query_key_value.weight with shape:  (4608, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.27.self_attention.query_key_value.bias with shape:  (4608,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.27.self_attention.dense.weight with shape:  (4096, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.27.post_attention_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.27.mlp.dense_h_to_4h.weight with shape:  (27392, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.layers.27.mlp.dense_4h_to_h.weight with shape:  (4096, 13696)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.encoder.final_layernorm.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.output_layer.weight with shape:  (65024, 4096)\n",
      "  Converting to float32\n",
      "Done. Output file: runtime_outs/ne_chatglm_f32.bin\n",
      "\n",
      "ne_ftype: 10\n",
      "ne_ftype: 10\n",
      "[   1/ 199] transformer.embedding.word_embeddings.weight -     4096 x 65024, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =  1016.00 MB ->   174.67 MB\n",
      "\n",
      "[   2/ 199] transformer.encoder.layers.0.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[   3/ 199] transformer.encoder.layers.0.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[   4/ 199] transformer.encoder.layers.0.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[   5/ 199] transformer.encoder.layers.0.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[   6/ 199] transformer.encoder.layers.0.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[   7/ 199] transformer.encoder.layers.0.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[   8/ 199] transformer.encoder.layers.0.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[   9/ 199] transformer.encoder.layers.1.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  10/ 199] transformer.encoder.layers.1.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[  11/ 199] transformer.encoder.layers.1.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[  12/ 199] transformer.encoder.layers.1.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  13/ 199] transformer.encoder.layers.1.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  14/ 199] transformer.encoder.layers.1.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[  15/ 199] transformer.encoder.layers.1.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[  16/ 199] transformer.encoder.layers.2.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  17/ 199] transformer.encoder.layers.2.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[  18/ 199] transformer.encoder.layers.2.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[  19/ 199] transformer.encoder.layers.2.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  20/ 199] transformer.encoder.layers.2.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  21/ 199] transformer.encoder.layers.2.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[  22/ 199] transformer.encoder.layers.2.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[  23/ 199] transformer.encoder.layers.3.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  24/ 199] transformer.encoder.layers.3.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[  25/ 199] transformer.encoder.layers.3.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[  26/ 199] transformer.encoder.layers.3.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  27/ 199] transformer.encoder.layers.3.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  28/ 199] transformer.encoder.layers.3.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[  29/ 199] transformer.encoder.layers.3.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[  30/ 199] transformer.encoder.layers.4.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  31/ 199] transformer.encoder.layers.4.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[  32/ 199] transformer.encoder.layers.4.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[  33/ 199] transformer.encoder.layers.4.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  34/ 199] transformer.encoder.layers.4.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  35/ 199] transformer.encoder.layers.4.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[  36/ 199] transformer.encoder.layers.4.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[  37/ 199] transformer.encoder.layers.5.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  38/ 199] transformer.encoder.layers.5.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[  39/ 199] transformer.encoder.layers.5.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[  40/ 199] transformer.encoder.layers.5.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  41/ 199] transformer.encoder.layers.5.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  42/ 199] transformer.encoder.layers.5.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[  43/ 199] transformer.encoder.layers.5.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[  44/ 199] transformer.encoder.layers.6.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  45/ 199] transformer.encoder.layers.6.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[  46/ 199] transformer.encoder.layers.6.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[  47/ 199] transformer.encoder.layers.6.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  48/ 199] transformer.encoder.layers.6.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  49/ 199] transformer.encoder.layers.6.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[  50/ 199] transformer.encoder.layers.6.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[  51/ 199] transformer.encoder.layers.7.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  52/ 199] transformer.encoder.layers.7.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[  53/ 199] transformer.encoder.layers.7.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[  54/ 199] transformer.encoder.layers.7.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  55/ 199] transformer.encoder.layers.7.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  56/ 199] transformer.encoder.layers.7.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[  57/ 199] transformer.encoder.layers.7.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[  58/ 199] transformer.encoder.layers.8.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  59/ 199] transformer.encoder.layers.8.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[  60/ 199] transformer.encoder.layers.8.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[  61/ 199] transformer.encoder.layers.8.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  62/ 199] transformer.encoder.layers.8.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  63/ 199] transformer.encoder.layers.8.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[  64/ 199] transformer.encoder.layers.8.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[  65/ 199] transformer.encoder.layers.9.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  66/ 199] transformer.encoder.layers.9.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[  67/ 199] transformer.encoder.layers.9.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[  68/ 199] transformer.encoder.layers.9.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  69/ 199] transformer.encoder.layers.9.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  70/ 199] transformer.encoder.layers.9.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[  71/ 199] transformer.encoder.layers.9.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[  72/ 199] transformer.encoder.layers.10.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  73/ 199] transformer.encoder.layers.10.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[  74/ 199] transformer.encoder.layers.10.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[  75/ 199] transformer.encoder.layers.10.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  76/ 199] transformer.encoder.layers.10.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  77/ 199] transformer.encoder.layers.10.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[  78/ 199] transformer.encoder.layers.10.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[  79/ 199] transformer.encoder.layers.11.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  80/ 199] transformer.encoder.layers.11.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[  81/ 199] transformer.encoder.layers.11.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[  82/ 199] transformer.encoder.layers.11.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  83/ 199] transformer.encoder.layers.11.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  84/ 199] transformer.encoder.layers.11.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[  85/ 199] transformer.encoder.layers.11.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[  86/ 199] transformer.encoder.layers.12.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  87/ 199] transformer.encoder.layers.12.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[  88/ 199] transformer.encoder.layers.12.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[  89/ 199] transformer.encoder.layers.12.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  90/ 199] transformer.encoder.layers.12.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  91/ 199] transformer.encoder.layers.12.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[  92/ 199] transformer.encoder.layers.12.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[  93/ 199] transformer.encoder.layers.13.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  94/ 199] transformer.encoder.layers.13.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[  95/ 199] transformer.encoder.layers.13.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[  96/ 199] transformer.encoder.layers.13.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  97/ 199] transformer.encoder.layers.13.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  98/ 199] transformer.encoder.layers.13.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[  99/ 199] transformer.encoder.layers.13.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[ 100/ 199] transformer.encoder.layers.14.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 101/ 199] transformer.encoder.layers.14.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[ 102/ 199] transformer.encoder.layers.14.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[ 103/ 199] transformer.encoder.layers.14.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 104/ 199] transformer.encoder.layers.14.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 105/ 199] transformer.encoder.layers.14.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[ 106/ 199] transformer.encoder.layers.14.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[ 107/ 199] transformer.encoder.layers.15.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 108/ 199] transformer.encoder.layers.15.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[ 109/ 199] transformer.encoder.layers.15.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[ 110/ 199] transformer.encoder.layers.15.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 111/ 199] transformer.encoder.layers.15.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 112/ 199] transformer.encoder.layers.15.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[ 113/ 199] transformer.encoder.layers.15.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[ 114/ 199] transformer.encoder.layers.16.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 115/ 199] transformer.encoder.layers.16.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[ 116/ 199] transformer.encoder.layers.16.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[ 117/ 199] transformer.encoder.layers.16.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 118/ 199] transformer.encoder.layers.16.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 119/ 199] transformer.encoder.layers.16.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[ 120/ 199] transformer.encoder.layers.16.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[ 121/ 199] transformer.encoder.layers.17.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 122/ 199] transformer.encoder.layers.17.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[ 123/ 199] transformer.encoder.layers.17.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[ 124/ 199] transformer.encoder.layers.17.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 125/ 199] transformer.encoder.layers.17.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 126/ 199] transformer.encoder.layers.17.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[ 127/ 199] transformer.encoder.layers.17.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[ 128/ 199] transformer.encoder.layers.18.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 129/ 199] transformer.encoder.layers.18.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[ 130/ 199] transformer.encoder.layers.18.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[ 131/ 199] transformer.encoder.layers.18.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 132/ 199] transformer.encoder.layers.18.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 133/ 199] transformer.encoder.layers.18.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[ 134/ 199] transformer.encoder.layers.18.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[ 135/ 199] transformer.encoder.layers.19.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 136/ 199] transformer.encoder.layers.19.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[ 137/ 199] transformer.encoder.layers.19.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[ 138/ 199] transformer.encoder.layers.19.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 139/ 199] transformer.encoder.layers.19.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 140/ 199] transformer.encoder.layers.19.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[ 141/ 199] transformer.encoder.layers.19.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[ 142/ 199] transformer.encoder.layers.20.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 143/ 199] transformer.encoder.layers.20.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[ 144/ 199] transformer.encoder.layers.20.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[ 145/ 199] transformer.encoder.layers.20.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 146/ 199] transformer.encoder.layers.20.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 147/ 199] transformer.encoder.layers.20.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[ 148/ 199] transformer.encoder.layers.20.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[ 149/ 199] transformer.encoder.layers.21.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 150/ 199] transformer.encoder.layers.21.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[ 151/ 199] transformer.encoder.layers.21.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[ 152/ 199] transformer.encoder.layers.21.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 153/ 199] transformer.encoder.layers.21.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 154/ 199] transformer.encoder.layers.21.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[ 155/ 199] transformer.encoder.layers.21.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[ 156/ 199] transformer.encoder.layers.22.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 157/ 199] transformer.encoder.layers.22.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[ 158/ 199] transformer.encoder.layers.22.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[ 159/ 199] transformer.encoder.layers.22.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 160/ 199] transformer.encoder.layers.22.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 161/ 199] transformer.encoder.layers.22.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[ 162/ 199] transformer.encoder.layers.22.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[ 163/ 199] transformer.encoder.layers.23.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 164/ 199] transformer.encoder.layers.23.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[ 165/ 199] transformer.encoder.layers.23.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[ 166/ 199] transformer.encoder.layers.23.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 167/ 199] transformer.encoder.layers.23.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 168/ 199] transformer.encoder.layers.23.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[ 169/ 199] transformer.encoder.layers.23.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[ 170/ 199] transformer.encoder.layers.24.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 171/ 199] transformer.encoder.layers.24.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[ 172/ 199] transformer.encoder.layers.24.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[ 173/ 199] transformer.encoder.layers.24.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 174/ 199] transformer.encoder.layers.24.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 175/ 199] transformer.encoder.layers.24.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[ 176/ 199] transformer.encoder.layers.24.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[ 177/ 199] transformer.encoder.layers.25.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 178/ 199] transformer.encoder.layers.25.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[ 179/ 199] transformer.encoder.layers.25.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[ 180/ 199] transformer.encoder.layers.25.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 181/ 199] transformer.encoder.layers.25.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 182/ 199] transformer.encoder.layers.25.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[ 183/ 199] transformer.encoder.layers.25.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[ 184/ 199] transformer.encoder.layers.26.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 185/ 199] transformer.encoder.layers.26.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[ 186/ 199] transformer.encoder.layers.26.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[ 187/ 199] transformer.encoder.layers.26.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 188/ 199] transformer.encoder.layers.26.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 189/ 199] transformer.encoder.layers.26.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[ 190/ 199] transformer.encoder.layers.26.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[ 191/ 199] transformer.encoder.layers.27.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 192/ 199] transformer.encoder.layers.27.self_attention.query_key_value.weight -     4096 x  4608, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    72.00 MB ->    12.38 MB\n",
      "\n",
      "[ 193/ 199] transformer.encoder.layers.27.self_attention.query_key_value.bias -             4608, type =    f32, 6_0_32_0_0,size =    0.018 MB\n",
      "\n",
      "[ 194/ 199] transformer.encoder.layers.27.self_attention.dense.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 195/ 199] transformer.encoder.layers.27.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 196/ 199] transformer.encoder.layers.27.mlp.dense_h_to_4h.weight -     4096 x 27392, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   428.00 MB ->    73.61 MB\n",
      "\n",
      "[ 197/ 199] transformer.encoder.layers.27.mlp.dense_4h_to_h.weight -    13696 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   214.00 MB ->    37.07 MB\n",
      "\n",
      "[ 198/ 199] transformer.encoder.final_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 199/ 199]      transformer.output_layer.weight -     4096 x 65024, type =    f32, 0_0_32_1_1,quantizing .. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.cpp: loading model from runtime_outs/ne_chatglm_f32.bin\n",
      "model.cpp: saving model to runtime_outs/ne_chatglm_q_int4_jblas_cint8_g32.bin\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from intel_extension_for_transformers.transformers import AutoModelForCausalLM, WeightOnlyQuantConfig\n",
    "woq_config = WeightOnlyQuantConfig(compute_dtype=\"int8\",weight_dtype=\"int4\",use_cache=True)\n",
    "model_name = \"chatglm3-6b\"     # Hugging Face model_id or local model\n",
    "prompt = \"Once upon a time, there existed a little girl,\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=woq_config, trust_remote_code=True)\n",
    "outputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc817e07-be11-4bd3-939f-e45f23af30d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T07:35:37.620748Z",
     "iopub.status.busy": "2024-06-02T07:35:37.620439Z",
     "iopub.status.idle": "2024-06-02T07:38:06.630677Z",
     "shell.execute_reply": "2024-06-02T07:38:06.630055Z",
     "shell.execute_reply.started": "2024-06-02T07:35:37.620727Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-02 15:36:17 [INFO] Applying Weight Only Quantization.\n",
      "2024-06-02 15:36:17 [INFO] Using LLM runtime.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd: ['python', PosixPath('/opt/conda/lib/python3.10/site-packages/intel_extension_for_transformers/llm/runtime/graph/scripts/convert_baichuan.py'), '--outfile', 'runtime_outs/ne_baichuan_f32.bin', '--outtype', 'f32', 'Baichuan2-7B-Chat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baichuan-13B converting: \n",
      "model.embed_tokens.weight torch.Size([125696, 4096]) torch.float32\n",
      "model.layers.0.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.1.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.1.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.2.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.2.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.3.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.3.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.4.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.4.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.5.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.5.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.6.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.6.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.7.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.7.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.8.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.8.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.9.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.9.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.10.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.10.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.11.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.11.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.12.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.12.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.13.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.13.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.14.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.14.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.15.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.15.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.16.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.16.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.17.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.17.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.18.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.18.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.19.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.19.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.20.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.20.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.21.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.21.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.22.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.22.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.23.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.23.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.24.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.24.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.25.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.25.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.26.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.26.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.27.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.27.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.28.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.28.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.29.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.29.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.30.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.30.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.31.self_attn.W_pack.weight torch.Size([12288, 4096]) torch.float32\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([4096, 11008]) torch.float32\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([11008, 4096]) torch.float32\n",
      "model.layers.31.input_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([4096]) torch.float32\n",
      "model.norm.weight torch.Size([4096]) torch.float32\n",
      "lm_head.weight torch.Size([125696, 4096]) torch.float32\n",
      "{'architectures': ['BaichuanForCausalLM'], 'auto_map': {'AutoConfig': 'configuration_baichuan.BaichuanConfig', 'AutoModelForCausalLM': 'modeling_baichuan.BaichuanForCausalLM'}, 'tokenizer_class': 'BaichuanTokenizer', 'bos_token_id': 1, 'eos_token_id': 2, 'hidden_act': 'silu', 'hidden_size': 4096, 'initializer_range': 0.02, 'intermediate_size': 11008, 'max_position_embeddings': 4096, 'model_max_length': 4096, 'model_type': 'baichuan', 'num_attention_heads': 32, 'num_hidden_layers': 32, 'pad_token_id': 0, 'rms_norm_eps': 1e-06, '_from_model_config': True, 'tie_word_embeddings': False, 'torch_dtype': 'bfloat16', 'transformers_version': '4.29.2', 'use_cache': True, 'vocab_size': 125696}\n",
      "Loading vocab file Baichuan2-7B-Chat/tokenizer.model\n",
      "Processing variable: model.embed_tokens.weight with shape:  (125696, 4096)\n",
      "Processing variable: model.layers.0.self_attn.W_pack.weight with shape:  (12288, 4096)\n",
      "Processing variable: model.layers.0.self_attn.o_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: model.layers.0.mlp.gate_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.0.mlp.down_proj.weight with shape:  (4096, 11008)\n",
      "Processing variable: model.layers.0.mlp.up_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.0.input_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.0.post_attention_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.1.self_attn.W_pack.weight with shape:  (12288, 4096)\n",
      "Processing variable: model.layers.1.self_attn.o_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: model.layers.1.mlp.gate_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.1.mlp.down_proj.weight with shape:  (4096, 11008)\n",
      "Processing variable: model.layers.1.mlp.up_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.1.input_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.1.post_attention_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.2.self_attn.W_pack.weight with shape:  (12288, 4096)\n",
      "Processing variable: model.layers.2.self_attn.o_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: model.layers.2.mlp.gate_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.2.mlp.down_proj.weight with shape:  (4096, 11008)\n",
      "Processing variable: model.layers.2.mlp.up_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.2.input_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.2.post_attention_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.3.self_attn.W_pack.weight with shape:  (12288, 4096)\n",
      "Processing variable: model.layers.3.self_attn.o_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: model.layers.3.mlp.gate_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.3.mlp.down_proj.weight with shape:  (4096, 11008)\n",
      "Processing variable: model.layers.3.mlp.up_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.3.input_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.3.post_attention_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.4.self_attn.W_pack.weight with shape:  (12288, 4096)\n",
      "Processing variable: model.layers.4.self_attn.o_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: model.layers.4.mlp.gate_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.4.mlp.down_proj.weight with shape:  (4096, 11008)\n",
      "Processing variable: model.layers.4.mlp.up_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.4.input_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.4.post_attention_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.5.self_attn.W_pack.weight with shape:  (12288, 4096)\n",
      "Processing variable: model.layers.5.self_attn.o_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: model.layers.5.mlp.gate_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.5.mlp.down_proj.weight with shape:  (4096, 11008)\n",
      "Processing variable: model.layers.5.mlp.up_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.5.input_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.5.post_attention_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.6.self_attn.W_pack.weight with shape:  (12288, 4096)\n",
      "Processing variable: model.layers.6.self_attn.o_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: model.layers.6.mlp.gate_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.6.mlp.down_proj.weight with shape:  (4096, 11008)\n",
      "Processing variable: model.layers.6.mlp.up_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.6.input_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.6.post_attention_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.7.self_attn.W_pack.weight with shape:  (12288, 4096)\n",
      "Processing variable: model.layers.7.self_attn.o_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: model.layers.7.mlp.gate_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.7.mlp.down_proj.weight with shape:  (4096, 11008)\n",
      "Processing variable: model.layers.7.mlp.up_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.7.input_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.7.post_attention_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.8.self_attn.W_pack.weight with shape:  (12288, 4096)\n",
      "Processing variable: model.layers.8.self_attn.o_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: model.layers.8.mlp.gate_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.8.mlp.down_proj.weight with shape:  (4096, 11008)\n",
      "Processing variable: model.layers.8.mlp.up_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.8.input_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.8.post_attention_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.9.self_attn.W_pack.weight with shape:  (12288, 4096)\n",
      "Processing variable: model.layers.9.self_attn.o_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: model.layers.9.mlp.gate_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.9.mlp.down_proj.weight with shape:  (4096, 11008)\n",
      "Processing variable: model.layers.9.mlp.up_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.9.input_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.9.post_attention_layernorm.weight with shape:  (4096,)\n",
      "Processing variable: model.layers.10.self_attn.W_pack.weight with shape:  (12288, 4096)\n",
      "Processing variable: model.layers.10.self_attn.o_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: model.layers.10.mlp.gate_proj.weight with shape:  (11008, 4096)\n",
      "Processing variable: model.layers.10.mlp.down_proj.weight with shape:  ne_ftype: 10\n",
      "[   1/ 146]            model.embed_tokens.weight -    4096 x 125696, type =    f32, 0_0_32_0_0,quantizing .. GGML size =  1964.00 MB ->   276.19 MB\n",
      "\n",
      "[   2/ 146] model.layers.0.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[   3/ 146] model.layers.0.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[   4/ 146]  model.layers.0.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[   5/ 146]  model.layers.0.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[   6/ 146]    model.layers.0.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[   7/ 146] model.layers.0.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[   8/ 146] model.layers.0.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[   9/ 146] model.layers.1.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[  10/ 146] model.layers.1.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  11/ 146]  model.layers.1.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  12/ 146]  model.layers.1.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  13/ 146]    model.layers.1.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  14/ 146] model.layers.1.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  15/ 146] model.layers.1.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  16/ 146] model.layers.2.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[  17/ 146] model.layers.2.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  18/ 146]  model.layers.2.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  19/ 146]  model.layers.2.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  20/ 146]    model.layers.2.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  21/ 146] model.layers.2.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  22/ 146] model.layers.2.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  23/ 146] model.layers.3.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[  24/ 146] model.layers.3.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  25/ 146]  model.layers.3.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  26/ 146]  model.layers.3.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  27/ 146]    model.layers.3.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  28/ 146] model.layers.3.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  29/ 146] model.layers.3.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  30/ 146] model.layers.4.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[  31/ 146] model.layers.4.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  32/ 146]  model.layers.4.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  33/ 146]  model.layers.4.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  34/ 146]    model.layers.4.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  35/ 146] model.layers.4.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  36/ 146] model.layers.4.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  37/ 146] model.layers.5.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[  38/ 146] model.layers.5.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  39/ 146]  model.layers.5.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  40/ 146]  model.layers.5.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  41/ 146]    model.layers.5.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  42/ 146] model.layers.5.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  43/ 146] model.layers.5.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  44/ 146] model.layers.6.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[  45/ 146] model.layers.6.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  46/ 146]  model.layers.6.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  47/ 146]  model.layers.6.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  48/ 146]    model.layers.6.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  49/ 146] model.layers.6.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  50/ 146] model.layers.6.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  51/ 146] model.layers.7.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[  52/ 146] model.layers.7.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  53/ 146]  model.layers.7.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  54/ 146]  model.layers.7.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  55/ 146]    model.layers.7.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  56/ 146] model.layers.7.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  57/ 146] model.layers.7.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  58/ 146] model.layers.8.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[  59/ 146] model.layers.8.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  60/ 146]  model.layers.8.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  61/ 146]  model.layers.8.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  62/ 146]    model.layers.8.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  63/ 146] model.layers.8.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  64/ 146] model.layers.8.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  65/ 146] model.layers.9.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[  66/ 146] model.layers.9.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  67/ 146]  model.layers.9.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  68/ 146]  model.layers.9.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  69/ 146]    model.layers.9.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  70/ 146] model.layers.9.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  71/ 146] model.layers.9.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  72/ 146] model.layers.10.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[  73/ 146] model.layers.10.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  74/ 146] model.layers.10.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  75/ 146] model.layers.10.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  76/ 146]   model.layers.10.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  77/ 146] model.layers.10.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  78/ 146] model.layers.10.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  79/ 146] model.layers.11.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[  80/ 146] model.layers.11.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  81/ 146] model.layers.11.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  82/ 146] model.layers.11.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  83/ 146]   model.layers.11.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  84/ 146] model.layers.11.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  85/ 146] model.layers.11.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  86/ 146] model.layers.12.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[  87/ 146] model.layers.12.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  88/ 146] model.layers.12.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  89/ 146] model.layers.12.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  90/ 146]   model.layers.12.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  91/ 146] model.layers.12.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  92/ 146] model.layers.12.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  93/ 146] model.layers.13.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[  94/ 146] model.layers.13.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[  95/ 146] model.layers.13.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  96/ 146] model.layers.13.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[  97/ 146]   model.layers.13.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[  98/ 146] model.layers.13.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[  99/ 146] model.layers.13.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 100/ 146] model.layers.14.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[ 101/ 146] model.layers.14.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 102/ 146] model.layers.14.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 103/ 146] model.layers.14.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 104/ 146]   model.layers.14.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 105/ 146] model.layers.14.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 106/ 146] model.layers.14.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 107/ 146] model.layers.15.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[ 108/ 146] model.layers.15.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 109/ 146] model.layers.15.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 110/ 146] model.layers.15.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 111/ 146]   model.layers.15.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 112/ 146] model.layers.15.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 113/ 146] model.layers.15.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 114/ 146] model.layers.16.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[ 115/ 146] model.layers.16.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 116/ 146] model.layers.16.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 117/ 146] model.layers.16.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 118/ 146]   model.layers.16.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 119/ 146] model.layers.16.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 120/ 146] model.layers.16.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 121/ 146] model.layers.17.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[ 122/ 146] model.layers.17.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 123/ 146] model.layers.17.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 124/ 146] model.layers.17.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 125/ 146]   model.layers.17.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 126/ 146] model.layers.17.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 127/ 146] model.layers.17.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 128/ 146] model.layers.18.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[ 129/ 146] model.layers.18.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 130/ 146] model.layers.18.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 131/ 146] model.layers.18.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 132/ 146]   model.layers.18.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 133/ 146] model.layers.18.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 134/ 146] model.layers.18.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 135/ 146] model.layers.19.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[ 136/ 146] model.layers.19.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 137/ 146] model.layers.19.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 138/ 146] model.layers.19.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.79 MB\n",
      "\n",
      "[ 139/ 146]   model.layers.19.mlp.up_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 140/ 146] model.layers.19.input_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 141/ 146] model.layers.19.post_attention_layernorm.weight -             4096, type =    f32, 6_0_32_0_0,size =    0.016 MB\n",
      "\n",
      "[ 142/ 146] model.layers.20.self_attn.W_pack.weight -     4096 x 12288, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   192.00 MB ->    33.00 MB\n",
      "\n",
      "[ 143/ 146] model.layers.20.self_attn.o_proj.weight -     4096 x  4096, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =    64.00 MB ->    11.09 MB\n",
      "\n",
      "[ 144/ 146] model.layers.20.mlp.gate_proj.weight -     4096 x 11008, type =    f32, 0_0_32_1_1,quantizing .. JBLAS size =   172.00 MB ->    29.65 MB\n",
      "\n",
      "[ 145/ 146] model.layers.20.mlp.down_proj.weight -    11008 x  4096, type =    f32, 0_0_32_1_1,quantizing .. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.cpp: loading model from runtime_outs/ne_baichuan_f32.bin\n",
      "model.cpp: saving model to runtime_outs/ne_baichuan_q_int4_jblas_cint8_g32.bin\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unexpectedly reached end of file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m      9\u001b[0m streamer \u001b[38;5;241m=\u001b[39m TextStreamer(tokenizer)\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwoq_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(inputs, streamer\u001b[38;5;241m=\u001b[39mstreamer, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/intel_extension_for_transformers/transformers/modeling/modeling_auto.py:173\u001b[0m, in \u001b[0;36m_BaseQBitsAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mintel_extension_for_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m    172\u001b[0m     model \u001b[38;5;241m=\u001b[39m Model()\n\u001b[0;32m--> 173\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43malg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheme\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_ggml\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_ggml\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gptq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_gptq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/intel_extension_for_transformers/llm/runtime/graph/__init__.py:123\u001b[0m, in \u001b[0;36mModel.init\u001b[0;34m(self, model_name, use_quant, use_gptq, **quant_kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP32 model will be used.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp32_bin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_bin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mquant_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(quant_bin), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFail to quantize model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# clean\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unexpectedly reached end of file"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from intel_extension_for_transformers.transformers import AutoModelForCausalLM, WeightOnlyQuantConfig\n",
    "woq_config = WeightOnlyQuantConfig(compute_dtype=\"int8\",weight_dtype=\"int4\",use_cache=True)\n",
    "model_name = \"Baichuan2-7B-Chat\"     # Hugging Face model_id or local model\n",
    "prompt = \"Once upon a time, there existed a little girl,\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=woq_config, trust_remote_code=True)\n",
    "outputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
